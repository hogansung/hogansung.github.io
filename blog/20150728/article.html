<h1 id="big-data-final-project-report">Big Data Final Project Report</h1>
<h2 id="titanic-survival-prediction">Titanic: Survival Prediction</h2>
<h3 id="this-is-a-general-statment-for-this-project.-it-means-nothing-actually.-what-i-want-to-do-is-just-to-increase-nonsenses-so-that-i-can-test-whether-genblog.py-works-normally-or-not.">This is a general statment for this project. It means nothing actually. What I want to do is just to increase nonsenses so that I can test whether genBlog.py works normally or not.</h3>
<h3 id="teammate">Teammate</h3>
<ul>
<li>R02922164 邵　飛</li>
<li>B00902064 宋昊恩</li>
<li>B00902048 吳瑞洋</li>
<li>B00902042 詹舜傑</li>
</ul>
<h3 id="data-information">Data Information</h3>
<h4 id="input">Input</h4>
<ul>
<li>Features
<ul>
<li>Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked.</li>
</ul></li>
<li>Features Meaning
<ul>
<li>Pclass: class of accommodation.</li>
<li>SibSp: number of sibling and spouse on board.</li>
<li>Parch: number of parents and children on board.</li>
<li>Ticket: ticket id.</li>
<li>Fare: fare paid.</li>
<li>Cabin: cabin accomodated.</li>
<li>Embarked: port of embarkation.</li>
</ul></li>
</ul>
<h4 id="output">Output</h4>
<ul>
<li>Single Label
<ul>
<li>Survived or Not Survived (1/0).</li>
</ul></li>
</ul>
<h4 id="data-size">Data Size</h4>
<ul>
<li>Number of Instance
<ul>
<li>Train: 891</li>
<li>Test: 418</li>
</ul></li>
<li>Number of Feature
<ul>
<li>Numerical: 4</li>
<li>Categorical: 3</li>
<li>Nominated: 3</li>
</ul></li>
</ul>
<h4 id="onboard-evaluation">Onboard Evaluation</h4>
<ul>
<li>Accuracy (TP/TP+FP)</li>
</ul>
<h3 id="tools-and-model-selection">Tools and Model Selection</h3>
<h4 id="tools">Tools</h4>
<ul>
<li>Pandas: Python package, used for <em>data manipulation</em></li>
<li>Sklearn: Python package, used for <em>data mining</em> and <em>data analysis</em></li>
</ul>
<h4 id="linear-model">Linear Model</h4>
<h5 id="logistic-regression">Logistic Regression</h5>
<ul>
<li>Model Introduction
<ul>
<li>One of the linear models, which is widely used to solve machine learning problems</li>
<li>Formula: <img src="http://i.imgur.com/6ZqrM6z.png" alt="Image Missing" style="width: 700px;"/></li>
<li>Figure: <img src="http://i.imgur.com/453A1l2.png" alt="Image Missing" style="width: 700px;"/></li>
</ul></li>
</ul>
<h5 id="linear-support-vector-machine">Linear Support Vector Machine</h5>
<ul>
<li>Model Introduction
<ul>
<li>Model will try to find out a hyperplane to separate data points in the space spanned by features.</li>
<li>Formula: <img src="http://i.imgur.com/s2RXx44.png" alt="Image Missing" style="width: 700px;"/></li>
<li>Figure: <img src="http://i.imgur.com/inZMFvB.png" alt="Image Missing" style="width: 700px;"/></li>
</ul></li>
</ul>
<h4 id="kernel-model">Kernel Model</h4>
<h5 id="support-vector-machine">Support Vector Machine</h5>
<ul>
<li>Model Introduction
<ul>
<li>Model will use RBF kernel to map data points into space with infinite dimension, then try to find out a hyperplane to separate data points.</li>
<li>Its performance should cover <em>Linear SVC</em>.</li>
<li>Formula: <img src="http://i.imgur.com/TRjiula.png" alt="Image Missing" style="width: 700px;"/></li>
<li>Figure: (mapping into a space with higher dimension) <img src="http://i.imgur.com/wFWyMis.png" alt="Image Missing" style="width: 700px;"/></li>
</ul></li>
</ul>
<h4 id="tree-based-model">Tree-based Model</h4>
<h5 id="gradient-boosting-classifier">Gradient Boosting Classifier</h5>
<ul>
<li>Model Introduction
<ul>
<li>Tree-based model with gradient descent update</li>
<li>Formula: <img src="http://i.imgur.com/WK31A5D.png" alt="Image Missing" style="width: 700px;"/></li>
<li>Figure: <img src="http://i.imgur.com/2bbYHQA.png" alt="Image Missing" style="width: 700px;"/></li>
</ul></li>
</ul>
<h5 id="random-forest-classifier">Random Forest Classifier</h5>
<ul>
<li>Model Introduction
<ul>
<li>Tree-based model, ensembled with many out-of-bag decision trees</li>
<li>Formula: <img src="http://i.imgur.com/m2WBB4r.png" alt="Image Missing" style="width: 700px;"/></li>
<li>Figure: <img src="http://i.imgur.com/AIfFgAZ.png" alt="Image Missing" style="width: 700px;"/></li>
</ul></li>
</ul>
<h5 id="adaboost-classifier">AdaBoost Classifier</h5>
<ul>
<li>Model Instruction
<ul>
<li>Selects only those features known to improve the predictive power of the model</li>
<li>Formula: <img src="http://i.imgur.com/e6ndBdA.png" alt="Image Missing" style="width: 700px;"/></li>
<li>Figure: <img src="http://i.imgur.com/sDlHf4A.png" alt="Image Missing" style="width: 700px;"/></li>
</ul></li>
</ul>
<h3 id="feature-engineering">Feature Engineering</h3>
<ul>
<li>Numerical
<ul>
<li>Features
<ul>
<li>Age, SibSp, Parch, Fare</li>
</ul></li>
<li>Preprocessing
<ul>
<li>Impute NA with mean value</li>
<li>Standard-scaling</li>
</ul></li>
</ul></li>
<li>Categorical
<ul>
<li>Features
<ul>
<li>Pclass, Sex, Embarked</li>
</ul></li>
<li>Preprocessing
<ul>
<li>No NA is discovered</li>
<li>Binary-feature Expansion</li>
</ul></li>
</ul></li>
<li>Nominated
<ul>
<li>Features
<ul>
<li>Name, Ticket, Cabin</li>
</ul></li>
<li>Preprocessing
<ul>
<li>Lots of NA value (ex: more than 90% NA in <em>Cabin</em>)</li>
<li>Hard to use without adding human knowledge (ex: <em>Name</em>)</li>
<li>We just eliminate them in this step</li>
</ul></li>
</ul></li>
</ul>
<h3 id="off-board-experiment-design">Off-board Experiment Design</h3>
<ul>
<li><p>Since there are few data for this problem, we must have a robust way to prevent overfitting. Then, we just apply 5-fold cross-validation for all model evaluation.</p></li>
<li><p>Though we are really careful about the overfitting problem, we still find out that there are 0.04 percent difference in accuracy between off-board and on-board.</p></li>
</ul>
<h3 id="model-performance-comparison">Model Performance Comparison</h3>
<h4 id="linear-model-1">Linear Model</h4>
<h5 id="logistic-regression-1">Logistic Regression</h5>
<ul>
<li>Best Parameters C=10, random_state=514</li>
<li>Performance</li>
</ul>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">Train</th>
<th align="center">Test</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Valid</td>
<td align="center">0.80387</td>
<td align="center">0.70020</td>
</tr>
<tr class="even">
<td align="center">Board</td>
<td align="center"></td>
<td align="center">0.76555</td>
</tr>
</tbody>
</table>
<h5 id="linear-svc">Linear SVC</h5>
<ul>
<li>Best parameters C=10, random_state=514</li>
<li>Performance</li>
</ul>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">Train</th>
<th align="center">Test</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Valid</td>
<td align="center">0.70078</td>
<td align="center">0.79460</td>
</tr>
<tr class="even">
<td align="center">Board</td>
<td align="center"></td>
<td align="center">0.75598</td>
</tr>
</tbody>
</table>
<h4 id="kernel-model-1">Kernel Model</h4>
<h5 id="support-vector-machine-1">Support Vector Machine</h5>
<ul>
<li>Best Parameters: C=1, gamma=0.125, random_state=514</li>
<li>Performance</li>
</ul>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">Train</th>
<th align="center">Test</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Valid</td>
<td align="center">0.80387</td>
<td align="center">0.70021</td>
</tr>
<tr class="even">
<td align="center">Board</td>
<td align="center"></td>
<td align="center">0.76555</td>
</tr>
</tbody>
</table>
<h4 id="tree-based-model-1">Tree-based Model</h4>
<h5 id="gradient-boosting-classifier-1">Gradient Boosting Classifier</h5>
<ul>
<li>Best Parameters estimator=500, depth=5, random_state=514</li>
<li>Performance</li>
</ul>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">Train</th>
<th align="center">Test</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Valid</td>
<td align="center">0.89870</td>
<td align="center">0.82041</td>
</tr>
<tr class="even">
<td align="center">Board</td>
<td align="center"></td>
<td align="center">0.77990</td>
</tr>
</tbody>
</table>
<h5 id="random-forest-classifier-1">Random Forest Classifier</h5>
<ul>
<li>Best Parameters: estimator=20, depth=5, random_state=514</li>
<li>Performance</li>
</ul>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">Train</th>
<th align="center">Test</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Valid</td>
<td align="center">0.85156</td>
<td align="center">0.82378</td>
</tr>
<tr class="even">
<td align="center">Board</td>
<td align="center"></td>
<td align="center">0.79904</td>
</tr>
</tbody>
</table>
<h5 id="adaboost-classifier-1">AdaBoost Classifier</h5>
<ul>
<li>Best Parameters estimator=30, depth=3, learning_rate= 0.2</li>
<li>Performance</li>
</ul>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">Train</th>
<th align="center">Test</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Valid</td>
<td align="center">0.85972</td>
<td align="center">0.83438</td>
</tr>
<tr class="even">
<td align="center">Board</td>
<td align="center"></td>
<td align="center">0.78469</td>
</tr>
</tbody>
</table>
<h4 id="comparison-from-figure">Comparison from Figure</h4>
<p><img src="http://i.imgur.com/PogzDRv.png" alt="Image Missing" style="width: 700px;"/></p>
<h3 id="model-ensemble">Model Ensemble</h3>
<ul>
<li><p>We choose the best answer collected from each model, including SVC, GBM, Random Forest and Adaboost, and aggregate them to gain on-board score <em>0.79904</em>, which is exactly the same as the <em>Random Forest</em> one.</p></li>
<li><p>One possible reason is that there is nearly nothing further can be learn from our current feature set, so different models have almost the same answer.</p></li>
<li><p>To have advanced score, we can either put more efforts on nominated features or try robust feature selection for each model to enhance the model exclusiveness.</p></li>
</ul>
<h3 id="conclusion">Conclusion</h3>
<ul>
<li><p>We implement six ML models in this <em>Titanic</em> problem and get 0.79904 as our best result. There are several points we learn from this competition, listed as follows:</p>
<ul>
<li><p>Some ML models have similar performance on one ML problem, i.e. Tree-based models.</p></li>
<li><p>Though some people make use of the well-known knowledge to gain 100 percent performance, this is not our main purpose in this competition. We just try to make use of what we have learned in this course.</p></li>
<li><p>There is a consistent gap between off-board and on-board score for all models. This may be caused by the imbalanced sampling in official data.</p></li>
</ul></li>
</ul>
<h3 id="reference">Reference</h3>
<ul>
<li><p>Python Package: Scikit Learn http://scikit-learn.org/stable/</p></li>
<li><p>Python Package: Pandas http://pandas.pydata.org/</p></li>
<li><p>Python Software for Convex Optimization - Documentation http://cvxopt.org/</p></li>
<li><p>A Library for Large Linear Classification http://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf</p></li>
<li><p>A Library for Support Vector Machine http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf</p></li>
<li><p>Wiki page for Support Vector Machine https://en.wikipedia.org/wiki/Support_vector_machine</p></li>
<li><p>Wiki page for Gradient Boosting https://en.wikipedia.org/wiki/Gradient_boosting</p></li>
<li><p>Wiki page for Random Forest https://en.wikipedia.org/wiki/Random_forest</p></li>
<li><p>Wiki page for AdaBoost https://en.wikipedia.org/wiki/AdaBoost</p></li>
</ul>
